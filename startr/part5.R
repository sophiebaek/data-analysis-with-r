# 머신러닝의 정의: 컴퓨터가 명시적으로 프로그램되지 않고도 학습할 수 있도록 하는 알고리즘과 기술을 개발하는 연구 분야
# result = f(data)

# 머신러닝의 종류:
# 1. 정답의 유무에 따른 구분
  # 1-1. 지도학습 (Supervised Learning): ex) 학습 분류, 회귀 모델
  # 1-2. 비지도학습 (Unsupervised Learning): ex) 군집화, 연관규칙분석 -> y가 없음
# 2. 학습 목적에 따른 구분
  # 2-1. 분류: ex) 로지스틱 회귀 분석, 나이브 베이즈, 의사결정나무, 인공 신경망, SVM -> 종속변수가 명목형 (categorical) 변수
  # 2-2. 회귀 (Regression): ex) 선형 회귀 분석, 인공신경망, SVR -> 종속변수가 연속형 (continuous) 변수
  # 2-3. 군집화 (Clustering): ex) K-Means 군집화, 계층적 군집화 -> 유사한 개체들의 집단을 판별
  # 2-4. 연관규칙분석 (Association Rule): -> 연관성이 높은 아이템들로 구성된 규칙 집합을 생성하는 방법론 / 추천시스템에 주로 사용 / 장바구니 분석이라고 불리기도 함

# 머신러닝 기반 추천 알고리즘:
# - User 기반 추천: 사용자와 비슷한 성향의 사용자들이 기존에 좋아했던 항목을 추천하는 방법
# - Item 기반 추천: 사용자가 기존에 좋아했던 항목과 유사한 특성을 지닌 항목을 추천하는 방법

##############################################################################

# 확률 용어:
# 확률 실험: 실험 결과를 미리 알 수 없지만, 발생 가능한 모든 결과는 알려져 있는 실험
# 표본 공간 (Sample space): 가능한 모든 결과들의 집합
# 사건 (Event): 표본 공간의 부분 집합
# 확률: 특정 사건이 발생할 가능성을 확률이라고 하며, 사건 E가 발생할 확률은 P(E)로 표기함

# 확률의 공리:
# 표본공간 S의 각 사건 E에 대하여 사건 E가 발생할 확률 P(E)는 아래 세 조건을 만족해야 함
# 1. 0 <= P(E) <= 1
# 2. P(S) = 1
# 3. 각 사건들이 서로 배반사건 (교집합이 없는 경우)일 때, 각 이벤트의 합집합의 확률 = 각 이벤트의 확률의 합

# 조건부 확률: 특정 사건 B가 발생했다는 가정 하에 사건 A가 발생할 확률을 조건부 확률이라고 하며, 사건 B에 대한 사건 A의 조건부 확률은 P(A|B)로 표기함

# 확률 변수: 표본 공간을 실수 값에 대응시키는 함수를 확률 변수라고 하며, 주로 X로 표기함
# - 이산 확률 변수: 확률 변수가 가지는 값을 셀 수 있는 경우
# - 연속 확률 변수: 확률 변수가 가지는 값을 셀 수 없는 경우

# 확률 분포: 확률 변수를 확률 값에 대응시키는 함수를 확률 분포라고 하며, 주로 f로 표기함
# - 확률 질량 함수: 이산 확률 변수의 확률 분포
# - 확률 밀도 함수: 연속 확률 변수의 확률 분포

##############################################################################

# 통계: 집단현상을 수량적으로 관찰하고 분석하는 것
# 모집단 (Population): 관심의 대상이 되는 전체 집합
# 표본집단 (Sample): 모집단의 일부분으로 관측을 통해 실제로 관측결과의 집합
# 모집단 -> 표본집단: 표본 추출
# 표본집단 -> 모집단: 추정

# 통계적 추정:
  # 모수 (Parameter): 모집단의 수치적 특성
  # 통계량 (Statistic): 표본에 따라 달라지는 표본집단의 수치적 특성
# 통계적 추정의 종류:
  # 점 추정 (point estimation)
  # 구간 추정 (interval estimation)

# 통계적 검정: 표본집단의 통계량을 기반으로 모집단에 대한 가설의 진위여부를 판단하는 것으로 통계적 가설 검정이라고 함
  # 귀무가설 H_0: 대립가설의 반대 가설
  # 대립가설 H_1: 검증하고자하는 가설
  # 단측 검정
    # 좌측 검정 -> H_1: mu < mu_0
    # 우측 검정 -> H_1: mu > mu_0
  # 양측 검정 -> H_1: mu != mu_0
# 검정 통계량의 값을 기준으로 귀무가설을 채택 or 기각함으로써 대립가설을 검증함
# 검정 통계량: 귀무가설 하에서 표본집단의 통계량을 기반으로 계산한 값
# 기각역: 유의 수준 alpha에서 귀무가설을 기각하는 통계량의 영역
# 유의 확률: 검정 통계량을 기준으로 귀무가설을 기각하게 하는 최소의 유의 수준으로 주로 p-value라고 함. p-value가 유의 수준 alpha보다 작으면 귀무가설을 기각함

##############################################################################

# Maximum Likelihood Estimation (MLE): MLE는 모수 (parameter)가 미지의 theta인 확률 분포에서 뽑은 표본 x을 바탕으로 표본의 likelihood를 최대화하는 방법을 통해 theta를 추정하는 기법
# Likelihood: 이미 주어진 표본적 증거에 비추어 보았을 때, 모집단에 관해 어떠한 통계척 추정이 그럴듯한 정도.
# 확률 분포 함수: 알려진 모수 mu = 0, variance = 1인 정규분포를 따르는 확률 변수 x의 함수
# 가능도/우도 함수: 미지의 mu와 variance = 1의 정규분포를 따르는 표본 x1 = 1, x2 = 2, x3 = 3에 대한 모수 mu의 함수

# MLE를 활용한 모수 추정: 미지의 mu와 variance = 1의 정규분포를 따르는 표본 x1 = 1, x2 = 2, x3 = 3를 기반으로 모수 mu를 추정함
# Likelihood와 log likelihood를 최대화하는 mu가 동일하므로 계산이 용이한 log likelihood로 변환하여 MLE를 진행함

##############################################################################

# Matrix 미분 표기법:
  # - Numerator layout: 미분 당하는 변수를 기준으로 미분 결과의 형태를 표기 (일반적으로 더 많이 사용)
  # - Denumerator layout: 미분 하는 변수를 기준으로 미분 결과의 형태를 표기

##############################################################################

# 선형 회귀 분석: 종속변수 y와 여러 독립변수의 집합 X 사이의 관계를 선형으로 가정하고, 해당 관계를 가장 잘 설명할 수 있는 모형을 찾는 분석 방법론 (y = f(X))
# 단순 선형 회귀 분석: 종속변수 y와 하나의 독립변수 x 사이의 관계를 선형 직선으로 표현한 후, 데이터를 가장 잘 표현할 수 있는 선형 회귀 직선의 회귀 계수를 추정함 (y = beta_0 + beta_1*x + epsilon -> white noise)
# 주어진 데이터를 설명할 수 있는 다양한 선형 직선 중 데이터를 가장 잘 표현할 수 있는 선형 회귀 직선의 회귀 계수를 추정해야함
  # 모집단의 회귀 직선: y = beta_0 + beta_1*x + epsilon
  # 추정할 회귀 직선: y_hat = beta_0_hat + beta_1_hat*x
# 단순 선형 회귀 분석 결과: 최소제곱법을 기반으로 데이터를 가장 잘 설명하는 회귀 직선을 추정한 결과, 단 하나의 회귀 직선이 도출됨

# 회귀 계수 beta_1_hat 해석: x가 1단위 증가할 때마다 y는 bete_1_hat 만큼 증가한다
# 회귀 계수 beta_1_hat 검정:
  # 귀무가설: H_0: beta_1 = 0 (변수의 설명력이 없다)
  # 대립가설: H_1: beta_1 != 0 (변수의 설명력이 존재한다)
