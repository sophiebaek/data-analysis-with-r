# 머신러닝의 정의: 컴퓨터가 명시적으로 프로그램되지 않고도 학습할 수 있도록 하는 알고리즘과 기술을 개발하는 연구 분야
# result = f(data)

# 머신러닝의 종류:
# 1. 정답의 유무에 따른 구분
  # 1-1. 지도학습 (Supervised Learning): ex) 학습 분류, 회귀 모델
  # 1-2. 비지도학습 (Unsupervised Learning): ex) 군집화, 연관규칙분석 -> y가 없음
# 2. 학습 목적에 따른 구분
  # 2-1. 분류: ex) 로지스틱 회귀 분석, 나이브 베이즈, 의사결정나무, 인공 신경망, SVM -> 종속변수가 명목형 (categorical) 변수
  # 2-2. 회귀 (Regression): ex) 선형 회귀 분석, 인공신경망, SVR -> 종속변수가 연속형 (continuous) 변수
  # 2-3. 군집화 (Clustering): ex) K-Means 군집화, 계층적 군집화 -> 유사한 개체들의 집단을 판별
  # 2-4. 연관규칙분석 (Association Rule): -> 연관성이 높은 아이템들로 구성된 규칙 집합을 생성하는 방법론 / 추천시스템에 주로 사용 / 장바구니 분석이라고 불리기도 함

# 머신러닝 기반 추천 알고리즘:
# - User 기반 추천: 사용자와 비슷한 성향의 사용자들이 기존에 좋아했던 항목을 추천하는 방법
# - Item 기반 추천: 사용자가 기존에 좋아했던 항목과 유사한 특성을 지닌 항목을 추천하는 방법

##############################################################################

# 확률 용어:
# 확률 실험: 실험 결과를 미리 알 수 없지만, 발생 가능한 모든 결과는 알려져 있는 실험
# 표본 공간 (Sample space): 가능한 모든 결과들의 집합
# 사건 (Event): 표본 공간의 부분 집합
# 확률: 특정 사건이 발생할 가능성을 확률이라고 하며, 사건 E가 발생할 확률은 P(E)로 표기함

# 확률의 공리:
# 표본공간 S의 각 사건 E에 대하여 사건 E가 발생할 확률 P(E)는 아래 세 조건을 만족해야 함
# 1. 0 <= P(E) <= 1
# 2. P(S) = 1
# 3. 각 사건들이 서로 배반사건 (교집합이 없는 경우)일 때, 각 이벤트의 합집합의 확률 = 각 이벤트의 확률의 합

# 조건부 확률: 특정 사건 B가 발생했다는 가정 하에 사건 A가 발생할 확률을 조건부 확률이라고 하며, 사건 B에 대한 사건 A의 조건부 확률은 P(A|B)로 표기함

# 확률 변수: 표본 공간을 실수 값에 대응시키는 함수를 확률 변수라고 하며, 주로 X로 표기함
# - 이산 확률 변수: 확률 변수가 가지는 값을 셀 수 있는 경우
# - 연속 확률 변수: 확률 변수가 가지는 값을 셀 수 없는 경우

# 확률 분포: 확률 변수를 확률 값에 대응시키는 함수를 확률 분포라고 하며, 주로 f로 표기함
# - 확률 질량 함수: 이산 확률 변수의 확률 분포
# - 확률 밀도 함수: 연속 확률 변수의 확률 분포

##############################################################################

# 통계: 집단현상을 수량적으로 관찰하고 분석하는 것
# 모집단 (Population): 관심의 대상이 되는 전체 집합
# 표본집단 (Sample): 모집단의 일부분으로 관측을 통해 실제로 관측결과의 집합
# 모집단 -> 표본집단: 표본 추출
# 표본집단 -> 모집단: 추정

# 통계적 추정:
  # 모수 (Parameter): 모집단의 수치적 특성
  # 통계량 (Statistic): 표본에 따라 달라지는 표본집단의 수치적 특성
# 통계적 추정의 종류:
  # 점 추정 (point estimation)
  # 구간 추정 (interval estimation)

# 통계적 검정: 표본집단의 통계량을 기반으로 모집단에 대한 가설의 진위여부를 판단하는 것으로 통계적 가설 검정이라고 함
  # 귀무가설 H_0: 대립가설의 반대 가설
  # 대립가설 H_1: 검증하고자하는 가설
  # 단측 검정
    # 좌측 검정 -> H_1: mu < mu_0
    # 우측 검정 -> H_1: mu > mu_0
  # 양측 검정 -> H_1: mu != mu_0
# 검정 통계량의 값을 기준으로 귀무가설을 채택 or 기각함으로써 대립가설을 검증함
# 검정 통계량: 귀무가설 하에서 표본집단의 통계량을 기반으로 계산한 값
# 기각역: 유의 수준 alpha에서 귀무가설을 기각하는 통계량의 영역
# 유의 확률: 검정 통계량을 기준으로 귀무가설을 기각하게 하는 최소의 유의 수준으로 주로 p-value라고 함. p-value가 유의 수준 alpha보다 작으면 귀무가설을 기각함

##############################################################################

# Maximum Likelihood Estimation (MLE): MLE는 모수 (parameter)가 미지의 theta인 확률 분포에서 뽑은 표본 x을 바탕으로 표본의 likelihood를 최대화하는 방법을 통해 theta를 추정하는 기법
# Likelihood: 이미 주어진 표본적 증거에 비추어 보았을 때, 모집단에 관해 어떠한 통계척 추정이 그럴듯한 정도.
# 확률 분포 함수: 알려진 모수 mu = 0, variance = 1인 정규분포를 따르는 확률 변수 x의 함수
# 가능도/우도 함수: 미지의 mu와 variance = 1의 정규분포를 따르는 표본 x1 = 1, x2 = 2, x3 = 3에 대한 모수 mu의 함수

# MLE를 활용한 모수 추정: 미지의 mu와 variance = 1의 정규분포를 따르는 표본 x1 = 1, x2 = 2, x3 = 3를 기반으로 모수 mu를 추정함
# Likelihood와 log likelihood를 최대화하는 mu가 동일하므로 계산이 용이한 log likelihood로 변환하여 MLE를 진행함

##############################################################################

# Matrix 미분 표기법:
  # - Numerator layout: 미분 당하는 변수를 기준으로 미분 결과의 형태를 표기 (일반적으로 더 많이 사용)
  # - Denumerator layout: 미분 하는 변수를 기준으로 미분 결과의 형태를 표기

##############################################################################

# 선형 회귀 분석: 종속변수 y와 여러 독립변수의 집합 X 사이의 관계를 선형으로 가정하고, 해당 관계를 가장 잘 설명할 수 있는 모형을 찾는 분석 방법론 (y = f(X))
# 단순 선형 회귀 분석: 종속변수 y와 하나의 독립변수 x 사이의 관계를 선형 직선으로 표현한 후, 데이터를 가장 잘 표현할 수 있는 선형 회귀 직선의 회귀 계수를 추정함 (y = beta_0 + beta_1*x + epsilon -> white noise)
# 주어진 데이터를 설명할 수 있는 다양한 선형 직선 중 데이터를 가장 잘 표현할 수 있는 선형 회귀 직선의 회귀 계수를 추정해야함
  # 모집단의 회귀 직선: y = beta_0 + beta_1*x + epsilon
  # 추정할 회귀 직선: y_hat = beta_0_hat + beta_1_hat*x
# 단순 선형 회귀 분석 결과: 최소제곱법을 기반으로 데이터를 가장 잘 설명하는 회귀 직선을 추정한 결과, 단 하나의 회귀 직선이 도출됨

# 회귀 계수 beta_1_hat 해석: x가 1단위 증가할 때마다 y는 bete_1_hat 만큼 증가한다
# 회귀 계수 beta_1_hat 검정:
  # 귀무가설: H_0: beta_1 = 0 (변수의 설명력이 없다)
  # 대립가설: H_1: beta_1 != 0 (변수의 설명력이 존재한다)

##############################################################################

# SST = SSR + SSE
# SST: Total sum of square
# SSR Regression sum of squares (회귀 직선으로 설명이 가능한 부분)
# SSE: Error sum of squares (회귀 직선으로 설명이 불가능한 부분)

# 결정계수: R^2은 회귀 모형의 적합도를 평가하기 위해 사용되는 대표적인 평가 지표임. 결정계수는 종속변수의 전체 변동 중 회귀 직선에 의해 설명되는 변동의 비율로 0~1의 범위를 가짐.
# R^2 = 1- SSE/SST = SSR/SST
# R^2 = 1: 회귀 직선으로 y의 총변동이 완전히 설명됨 (모든 표본들이 회귀 직선 위에 있음)
# R^2 = 0: 추정된 회귀 직선은 x와 y의 관계를 전혀 설명하지 못함
# 수정 결정계수: 기존 결정계수는 유의하지 않은 변수가 추가되어도 항상 증가하는 문제가 존재하므로 이러한 문제점을 보완하기 위한 수정 결정계수를 통해 보다 정확하게 회귀 모형의 적합도를 평가함
  # 변수의 개수에 대한 penalty를 추가해 유의하지 않은 변수가 추가될 경우 결정계수가 증가하지 않도록 함

# 선형 회귀 모델의 기본 가정: 회귀 분석에서는 잔차 (noise)에 대한 아래 3가지 가정이 존재함. 잔차분석을 통해 적합된 회귀 모델이 해당 가정을 잘 만족하는지 확인함
  # 1. 정규성: 잔차의 분포가 평균이 0인 정규분포를 따름
  # 2. 독립성: 잔차는 서로 독립적임
  # 3. 등분산성: 잔차의 분산이 동일함

# 선형 회귀 분석의 진단: 일반적으로 잔차 plot, Q-Q plot, residual vs fitted plot를 이용하여 잔차의 가정을 진단하며, 기본 가정이 위배된 경우 변수 변환을 통해 문제를 완화함
  # 잔차 plot: 규칙성이 없이 랜덤하여야함.
  # Q-Q plot: 일직선일수록 정규성을 따름

##############################################################################

# 다중 선형 회귀 분석: 독립변수가 1개인 단순 선형 회귀 분석과 다르게 종속변수 y와 여러 독립변수의 집합 X 사이의 관계를 선형으로 가정하고, 해당 관계를 가장 잘 설명할 수 있는 모형을 찾는 분석 방법론
# Matrix: 
  # Y -> nx1
  # X -> nx(p+1) 첫번째 column이 1로 채워져있음
  # Beta -> (p+1)x1
  # epsilon -> nx1

# 다중 선형 회귀 계수 추정: 단순 선형 회귀 계수 추정과 동일하게 다중 선형 회귀에서도 잔차 (residual)의 제곱합 (SSE)를 최소화하는 회귀 계수를 추정함
# Beta_hat = (X^TX)^-1 X^TY

# 다중 선형 회귀 계수의 의미: 
  # 회귀 계수 Beta_1_hat 해석: 다른 모든 독립변수의 값이 고정되어 있을 때, x1의 1단위 증가할 때마다 y는 Beta_1_hat 만큼 증가한다

# 다중 선형 회귀 분석의 검정:
  # 귀무가설: H_0: beta_1 = 0 (변수의 설명력이 없다)
  # 대립가설: H_1: beta_1 != 0 (변수의 설명력이 존재한다)

##############################################################################

# 다중공선성 (Multicollinearity): 다중공선성은 일부 설명 변수가 다른 설명 변수와 상관관계 정도가 높아 회귀 분석시 부정적인 영향을 미치는 현상임
# 독립변수 사이의 강한 상관관계 때문에 독립변수가 종속변수를 설명하는 변동성이 겹쳐서 다중공선성이 발생함 -> 잘못된 변수 해석, 예측 정확도 하락

# 다중공선성 진단: Variance Inflation Factor (VIF)
# VIF는 다른 변수의 선형 결합을 통해 특정 설명 변수를 설명할 수 있는 정도를 나타냄
  # Step 1: x_i를 종속 변수, 나머지 변수들을 독립변수로 하여 회귀 모형을 적합
  # Step 2: 회귀 모형의 결정계수 (R_i)^2를 기반으로 VIF_i 산출
    # - 일반적으로 VIF가 10 이상인 경우 다중공선성이 있는 변수라고 판단함

##############################################################################

# 회귀 모형 성능 평가: 회구 모형의 예측력을 평가하기 위해 예측 값과 실제 값이 유사한지 평가하는 척도가 필요함. 대표적으로 아래 5가지의 척도를 사용하여 모델의 성능을 평가함
  # 1. Average Error: 실제 값에 대한 예측 값의 과대/과소 추정 정도
  # 2. Mean Absolute Error (MAE): 실제 값과 예측 값 사이의 절대적인 오차의 평균
  # 3. Mean Absolute Percentage Error (MAPE): 실제 값 대비 실제 값과 예측 값 사이의 절대적인 오차의 평균. 상대적인 오차를 추정하기 위해 주로 사용됨
  # 4. Mean Squared Error (MSE = SSE): 실제 값과 예측 값 사이의 오차 제곱의 평균
  # 5. Root Mean Squared Error (RMSE): MSE에 root를 취한 값

##############################################################################

# 변수간 독립성이 만족되면 변수의 개수가 증가할수록 모델의 성능이 향상되지만, 형실에서는 변수의 개수가 일정 수준 이상 증가하면 모델의 성능이 저하되는 경향이 있음. 따라서 아래의 방법론을 기반으로 최적의 변수 조합을 찾음
  # 1. 전진 선택법 (Feedforward Selection): 설명 변수가 하나도 없는 모델에서부터 시작하여 가장 유의미한 변수를 하나씩 추가해 나가면서 최적의 변수 조합을 찾는 방법
    # 종료 시점: 어떠한 변수를 추가해도 변수 선택 지표의 향상이 없는 지점
    # 단점: 이미 선택한 변수는 다시 뺄 수 없음
  # 2. 후진 선택법 (Backward Selection): 모든 설명 변수를 사용하여 구축한 모델에서부터 시작하여 가장 유의미하지 않은 변수를 하나씩 제거해 나가면서 최적의 변수 조합을 찾는 방법
    # 종료 시점: 어떠한 변수를 제거해도 변수 선택 지표의 향상이 없는 지점
  # 3. 단계적 선택법 (Stepwise Selection): 설명 변수가 하나도 없는 모델에서부터 시작하여 전진 선택법과 후진 소거법을 번갈아가며 수행하면서 최적의 변수 조합을 찾는 방법
    # 장점: 이미 선택한 변수가 빠질 수도 있음

# 변수 선택 평가 지표: 일반적으로 아래 3가지 지표 중 1개를 선택하여 변수 선택을 위한 평가 지표로 사용함
  # 1. Akaike Information Criteria (AIC) - 낮을수록 좋음
  # 2. Bayesian Information Criteria (BIC)
  # 3. 수정 결정계수